---
layout: post
title: Practice 2 – 3D reconstruction
subtitle: Robot Implementation
social-share: false
readtime: true
---

**Three-dimensional (3D) object reconstruction** is an essential tool for many robotic (and computer vision) applications and, therefore, its understanding is essential for any computer vision engineer. Conceptually, 3D reconstruction is the process of accurately modelling the shape and depth of real objects from information that is not originally 3D.  

The use of 3D reconstruction in robotics will, therefore, give detailed insights of the scene it belongs to, allowing us to navigate on it and auto-localize ourselves. 

In this sense, the second practice proposed for the Robotics’ Vision Course is intended to develop a software program able to generate a scene 3D reconstruction.  Precisely, we will focus on a special case of 3D reconstruction problems: **the stereo reconstruction**.

Stereo 3D reconstruction begins with a stereo pair (i.e. two images of the same region taken from different positions) and, based on them the 3D points of the scene will be modeled. To reach this result, the following reconstruction algorithm is proposed:
1.	Important feature points extraction in one of the images.
2.	Detect the corresponding feature points in the other image.
3.	Triangulate in 3d space.

Along this blog entry we will discuss these steps theoretically, considering all the minutiae that they entail, and how they were implemented practically. 

Let’s get on duty!

> As in previous practices, the [Unibotics platform]( https://unibotics.org/) will be used  for the development of the 3D reconstruction algorithm. The Robotics Academy Docker Image is already installed in our machine, hence, in order to get better simulation metrics, the exercise will be run locally.


## Development

As introduced, our goal is to generate a 3D reconstruction of a scene based on two images (right and left) extracted from two parallel cameras placed on the *kobuki robot*.  Specifically, the scene aimed to be reconstructed is shown on the stereo pair images displayed below these lines. As seen, we are dealing with a synthetic scene containing Mario Bros characters. 

![Original set of images](https://github.com/brodgon/brodgon.github.io/blob/master/docs/imagen_inicial_a_reconstruir.png?raw=true)

These images will be the starting point for our algorithm. From them, the scene will be reconstructed. In this section, the sequence of steps taken is described.


### STEP 1: Relevant Feature Point Detection
The first step for our reconstruction algorithm is to detect the points we want to focus our attention on and, therefore, reconstruct. Due to computational and time constraints, we can’t employ an algorithm using the complete image (i.e., taking all its pixels), instead the most important points (pixels) need to be selected. With this purpose, lots of classic algorithms are found on literature, including SIFT, SURF or Harris algorithms, among others; however, the use of any of these methods will result on an insufficient number of points to carry out the reconstruction. Therefore, an equilibrium between these approaches needs to be found.

As proposed on the [exercise’s guidelines]( https://unibotics.org/academy/exercise/3d_reconstruction/), one option to tackle this problem is using edge points as our feature points. To fulfill this step, a popular edge detector was used: the **Canny Edge Detector**. Canny edge detector is a classic technique aimed to detect the structural information of different objects present on a given image. This way, enough points are extracted to get a nice reconstruction without spending a big amount of time on it. 

The application of Canny Detector to our images is straightforward since OpenCV has an implementation on it (`cv2.Canny()`) in which we just need to specify the image to detect borders (in grayscale) and the minimum and maximum values (set in 90 and 255 respectively) for borders extraction. This method returns a binary image in which only edges are maintained white.  An example of the output image is displayed below.

![Edges](https://github.com/brodgon/brodgon.github.io/blob/master/docs/bordes.png?raw=true)

Once we have extracted the binary image, we will extract the interest points, just searching those pixels coordinates in which its intensity is different from zero (i.e. white pixels).  This results in 11212 points to process; a number which will give us enough information to process and obtain a nice reconstruction. 

It is important to mention that, in order to get reduce the number of points and to avoid preserving every detail of the image, a bilateral filter was applied to the original images prior to the edge detector.


### STEP 2: Searching for matching point

Once the feature points are obtained, the next step is to obtain their **homologous** on the counter image; that is, starting with the features of interest of one of the images, we need to find its matching point on the other one.  

The easiest way of approaching this problem is by selecting a neighborhood in one of the images, move it to the other and walk wholly through it until we obtain the most similar patch to our template. This approach may work for small images or in scenarios in where a small number of points need to be processed. However, since we are dealing with big images and a big number of points, this solution is not the best for us. 

In order to get away from this problematic, the **epipolar constraint** is introduced. According to the **epipolar geometry** when two cameras watch the same scene from two different positions (stereo vision), there are strong geometric relationships between their points. In this sense, it is stated that, when a point is projected on both images, each point observed in one image must be observed in the counter one on a given **epipolar line**. 

With this in mind, is easy to think that, if we know the point we are searching for is placed around a line (we will set a fringe to avoid possible calibration errors), we should just look around it, reducing searching time and computational costs. This assumption is correct, but what is the **epipolar line**? 

Theoretically, the epipolar line is the intersection between the **epipole** (i.e., point of intersection between the camera centers and image plane) and the image plane or, shortly, the straight line defined by the projection of the center and 3D point of one of the cameras into the other.

With this understood, we have all the needed tools to start our homologous search!

The first step will be to define a reference image (i.e., the image from which features points to match are extracted) and a search image (i.e., the image in which we will look for correspondence). Randomly, we chose the left image as the reference and the right one for searching. Having this clear, we will look for the homologous points of the features of interest extracted from the left image in **STEP 1** in the right one.

Next step will be to compute the right epipolar line in which the homologous point for each of the feature points falls. To do this, two points on the right image need to be selected. In this sense, the procedure to follow is simple. Precisely, for each of the selected 2D points (on left image):
1.	Flip X and Y coordinates for coordinate coherence with the simulator functions.
2.	Transform cartesian coordinates into homogenous coordinates.
3.	Transform coordinates into the Camera reference system.
4.	Backproject the point of interest into the 3D scene (using `HAL.backproject()` function).
5.	Transform left camera center coordinates (3D) into homogeneous coordinates.
6.	Get middle point between the backprojection and camera center (subtraction between camera center and backprojeted point).
7.	Project both 3D points onto the right camera (using `HAL.project()` function)
8.	Transform back to image coordinate system.

After running this, let’s say, preprocessing stage, we will have two 2D points (in homogeneous coordinates) which will allow us to define the epipolar line. These points are shown on the image right below this line. Precisely, backprojected and projected original point is shown green while the projection of the left center camera is shown in blue.

![Points to define epipolar](https://github.com/brodgon/brodgon.github.io/blob/master/docs/back_projected_point_and_camera.png?raw=true)

 Mathematically, given two points in homogenous coordinates, we can easily define the slope and intercept line that joins them by operating on the cross-product of the points. Precisely,

$$
p = (x_{1}, y_{1}, 1) \times (x_{2}, y_{2}, 1) \quad slope = -\frac{p_{x}}{p_{y}} \quad intercept = - \frac{p_{z}}{p_{y}}
$$

Having these two lines parameters, we have defined the epipolar line; however, we want to define the pixels coordinates through which this line falls to perform our matching in next steps. In order to get them, we will generate a binary mask using the OpenCV line function (cv2.line()) function. Precisely, we will draw on a black image a white line defined by the slope and intercept previously calculated. 

At this point is important to mention that, in order to avoid noise or possible calibration errors, the line is calculated with certain thickness, that is, instead of working with a single line we will be working with a stripe or, according to our code, the **epipolar mask**. Below this lines, the epipolar mask for the selected left point is displayed.

![Epipolar mask](https://github.com/brodgon/brodgon.github.io/blob/master/docs/epipolar_stripe.png?raw=true)

Having the point of interest on the left image together with the search area on the right one, we are ready to search its homologous point. Just as recommended on the exercise’s guidelines, we will use cv2.templateMatching() for this purpose. According to OpenCV’s [documentation]( https://docs.opencv.org/3.4/d4/dc6/tutorial_py_template_matching.html) this function provides a method for searching and find the location of a given template on a larger image just by giving the reference template, the image we want to look on and a metric. 

In our specific case, the template will be defined as the 3x3 neighborhood around the 2D point of interest on the left image, while the search region will be given by masking the right image with the epipolar mask calculated previously (using cv2.bitwise_and() method). Additionally, the **square difference metric** will be used since, subjectively, throws the best results.

After the search, we will get the location in which our metric was **minimize** and define it our homologous point. Figure below shows the result for one of the points.

![Homologous matching](https://github.com/brodgon/brodgon.github.io/blob/master/docs/homologous.png?raw=true)

Having the original point on the left image together with its homologous on the right, we have all the information needed to get the 3D reconstruction for such point.

>  Notice this procedure was exemplified for a single point, however, it needs to be repeated for all of the points selected.

### STEP 3: Triangulation

Epipolar geometry states that given two homologous points on a stereo pair  (xL and xR) corresponding to the same 3D point, their projection beams should intersect precisely on such 3D point. Therefore, if the coordinates of two images points are known, its corresponding 3D point can be guessed by a process called **triangulation**.

As mentioned, the 3D point will be simply given by the intersection of the beams defined by xL and left camera center and xR and right camera center respectively. Therefore, our first step will be to backproject the homologous pair of interest onto the 3D space and, with them, define the director vectors of left and right beams respectively. Having this, the 3D point will be given by the intersection point of both beams.

Although intuitively this assumption is correct, we must think outside the box and consider all the possible mistakes we have made during the process, including small errors by forcing ourselves to work with integers values, or possible mistakes on the point matching. Having this in mind, is easy to see that our beams may not perfectly intersect.

However, this is not a big problem, since there are a lot of methods to tackle this problem. In our case we decided to compute the points of the beams for which distance is minimized. If lines intersect perfectly, distance will be zero and, therefore, the resulting point on each beam will be the same. 

To implement this solution [this link](https://math.stackexchange.com/questions/28503/how-to-find-intersection-of-two-lines-in-3d) together with [this]( https://stackoverflow.com/questions/74538331/finding-the-intersection-of-two-lines-in-3d-space) were followed. 

As stated on the previous references, when we have two pair of points, each defining a line, its easy to get the line to which they belong in their vector or parametric form. Assumuming xL = (xL_{1}, xL_{2}, xL_{3}) anc CL=(CL_{1}, CL_{2}, CL_{3}) define the first line, while xR = (xR_{1}, xR_{2}, xR_{3}) and CR = (CR_{1}, CR_{2}, CR_{3}) the second, it is stated that beams will only cross if there is a solution for t and s (sacalars) on the next system:

$$
CL_{1}+t(xL_{1}−CL_{1})=CR_{1}+s(xR_{1}−CR_{1})

CL_{2}+t(xL_{2}−CL_{2})=CR_{2}+s(xR_{2}−CR_{2})

CL_{3}+t(xL_{3}−CL_{3})=CR_{3}+s(xR_{3}−CR_{3}).
$$

By solving this system by the least square apporximation, we will get the values of t and s for which, when clearing values for these parameters in the above equation, we will obtain the points of each of the lines which have the minimum distance. Therefore, by computing the mean between both points, we will get our 3D point ready to be plotted in the unibotics viewfinder (in which x (red), y (green) and z (blue) axes are represented). 3D points will be plotted one by one using the `GUI.ShowNewPoints()` function, maintaining their original color extracted from left image. Notice that, in case points are the same, taht is, there is a perfect intersection, we will get the same values after performing the mean.

As a fact, not all points were plotted, we just considered those in which the homologous point was a feature of interest in the right image or a neighbor of it.

## Obtained Results 

Results are illustrated, from different views, on the following images.

![Frontal view](https://github.com/brodgon/brodgon.github.io/blob/master/docs/frontal.png?raw=true)

*Results seen frontally.*

![Lateral](https://github.com/brodgon/brodgon.github.io/blob/master/docs/lateral.png?raw=true)

*Results seen laterally.*

![Top view](https://github.com/brodgon/brodgon.github.io/blob/master/docs/superior.png?raw=true)

*Results seen from top view.*


Additionally, if you want to see this results in movement, a video around the reconstruction is found by clicking on [this link](https://urjc-my.sharepoint.com/:v:/g/personal/b_rodriguezg_2018_alumnos_urjc_es/EeuwJFMrEmxAriepP4e45nkBW0VKChXtDPVC2SGWDr2kaQ?e=cir1by). 

## Discussion and Improvements

As seen on the above results, a decent 3d construction has been obtained; however, results are not perfect. 3D reconstruction is really dependent on the number of details we want to model. If we take a closer look to the above images, we can see that areas in which relatively small detail is preserved in edges detection (like the big cereal box placed on the left) the 3D reconstruction is accurate, showing nice and clean results. Nonetheless, if we take a closer look to those places in which high detail was mantained (like the cube pyramid on the right), although we can get an insight on the shape, we can't really identify perfectly the little cubes since a lot of noise is added. 

Noise may have been reduced by defining a smaller epipolar fringe or considering less details (borders) frome the original image. These aspects will need to be optimize if our final application requires more accurate 3D reconstructions.

Nevertheless, the objectives of the second practice of the Robotic Vision course have been fullfilled, since results have been obtained and theory has been understood.

For any coments or doubts, don't hesitate to contact me!!

See you soon with P3!!



> **Note:** This post was created on the 19/04/2023, being its last edition the 20/04/2023
