---
layout: post
title: Practice 3 – Implementation of a robot autolocalization algorithm
subtitle: Based on beacon detection 
social-share: false
readtime: true
---

Robot **autolocalization** is an essential tool for a variety of robotic applications, since it will allow the robot to determine its position and orientarion with respect of the rest of the world. This information, besides allowing robots to construct maps of the environment, it will also take part in the decision-making process (depending on where the robot is, it will make one or other action).  

Robot autolocalization may be achieved by different means, including probabilistic and geometric approaches among others. In this practice we will focus our attention on **geometric methods using beacons**. Shortly, this techinique consist on the detection of beacons (randomly grouped white and black pixels) placed on strategic positions of the space. Beacons' information will be captured using **cameras** (cameras are our best tools) and basic processing techniques, which will allow us to set up a system of equations that will allo us to determine robot's position.

The objective of this practice is to implement a toy model for robot autolocalization based on beacons using our mobile phone camera. Precisely, three localization scenarios were studied:
1. Based on single images.
2. Based on 'pre-recorded' videos.
3. Based on 'real-time' videos.

Although these three scenarios use different *data sources*, the heart of the algorithm is the same. Along this blog entry we will discuss the process taken, considering all the minutae it entails.

Let’s get on duty!

> In ths practice, the implemented code was completely developed and execute on a local machine (running in Windows 11), using Pycharm 2022.2.2.

## Methodology

As introduced, our goal is to autolocalize our mobile phone in the three-dimensional space using beacons. To reach our objetive, several steps need to be taken:

1. Camera calibration: since we need the camera matrix to solve our problem.
2. Beacon's corners detection.
3. Camera's position estimation
4. Results visualization.

In this section, we will go through each of the steps. Nonetheless, the first real step before really starting to code, is to collect all the neccesary materials (i.e, a camera, a callibration pattern and beacons). In this sense, a 4x4 beacon (ID 11) was downloaded and printed from the [Online ArUco markers genearator](https://chev.me/arucogen/) together with an OpenCV's 9x6 chessboard calibration pattern. With this and our mobile phone camera (in my case I will be using a Xiaomi Mi A3) we are ready to start.

### STEP 1: Camera calibration

To correctly and easily implement our algorithm, the camera used needs to be properly calibrated (i.e., we need to extract its intrinsics and extrinsics parameters). For this, a calibration process based on a plane template captured from N different images (and therefore positions), will be held. To run it, we need to take different photos from the chosen chessboard. At this point is essential to understand that images used for callibrations must have the same size (or at least same aspect ratio) than the video size our phone can captured. In our case, everything was set to a 19:6 ratio. This step is essential to avoid misunderstandings in the next steps. 


![Chessboard images for calibration](https://github.com/brodgon/brodgon.github.io/blob/master/docs/chessboard.png?raw=true)
*Calibration pattern images**

Having the 20 images that we will be using in the process, the camera will be calibrated. After reading the images, chessboard's corners pixels' positions will be detected and cleaned and absolute real positions will be determined (manually, measuring the dimensions of each chessboard box). With this information, camera will be calibrated using OpenCV's function `cv2.calibrateCamera`, which will return intrinsics, extrinsics (rotation and traslation vectors), distortion coefficients and an error metric. All this process will be encapsulated in a function named **`camera_calibration`** which  , given a path to a driectory containing the N images, will return the intrinsics and distortion coefficients of the used camera (is all we need for later steps.

> **Note**: The implemented code for this steps consists on an adaptation from pieces of code given by [Jose Miguel Buenaposada](https://mastervisionartificial.es/jose-miguel-buenaposada-biencinto) in the [3D Vision Course of this same master's degree](https://mastervisionartificial.es/asignaturas/vision-tridimensional) and completed by the author of this blog.

### STEP 2: Beacon's corners detection

Once our camera is completely characterized, the next step is to detect the beacon we have printed. OpenCV (contributions) makes this process extremely simple since it has a module (`cv2.aruco`) implemented just for this purpose. To make it operate we will first need to define a dictionary containing the main information of the ArUco marker to detect (that is, the number of boxes) using `cv2.aruco.getPredefinedDictionary()` with the flag `cv2.aruco.DICT_4X4_50` (since we are using a 4x4 marker). With this, and defining a detector, the four corners of the marker (begining by upper left corner) will be detected. This information will be printed on the image using different colors and pixel's (2D) positions will be saved.  This functionality is encapsulated on a function named `detect_beacon()`.

The resulting image is shown below these lines. For simplicity, each corner was assigned a color; precisely (left upper corner will be represented by a green dot, left bottom corner by a blue one, right upper corner in cyan and right lower corner in yellow).


![Detection](https://github.com/brodgon/brodgon.github.io/blob/master/docs/detection_beacon.png?raw=true)
*Corners detection on a beacon*

If no beacon is detected, all positions will be set to (0,0).


### STEP 3: Estimating camera's position

With the camera calibrated and the set of 2D points, the next step is to estimate the position of the camera based on the position of the beacon. This is possible thanks to the **Perspective-n-Point (PnP)** method; a widely accepted technique to estimate camera pose based on **3D points** and their **2D points projections** in the image. 

At this point, we have talk about 2D-points.... how can we defined 3D points?? Three dimensional points are just the real points we are visualizing (i.e., real-life measurements), therefore, we just have to take a ruler and measure!!. We know that our Aruco markers has a side length of 79mm, therefore, by setting the reference system in the top left corner (i.e., top left corner will be (0,0,0)) and setting the Z coordinate of our points to 0 since we are working on a plane, we can easily make correspondance between pairs.

Having guaranteed the proper relation between 3D and 2D points (it is essential they are defined on the same order to avoid future errors), we have all the elements to estimate the camera position using PnP. Although this technique is tremendously difficult, since it allows to solve an over-determined system of equations; it's use is pretty straight-forward since OpenCV has an implementation of it (`cv2.solvePnP()`). This method returns the **rotation and traslation vectors** that represents camera position just by introducing the pairs of 3D-2D points, camera intrinsics matrix and the distortion coefficients.

Although we have calculated the rotation and translation vectors defining the camera, we still have to get the camera pose as a 3D point. To do this, the following eqution will be applied:

$$  C = A^{-1}b \quad where \quad A = KR \quad and \quad b = Kt $$

This will return the position of the camera with respect to the marker and with this, we have all our elements to visualize!!!

> **Note**: The rotation matrix used for estimating the camera position has been calculated by applying Rodrigues to the rotation vectors returned by OpenCV PnP method.


### STEP 4: Visualization

The hard work is done! Now we just need to visualize it. In sense, we will activate `matplotlib's` interactive framework for 3D visualization in order to *navigate* in the plots. Matplotlib's have numerous functionalities for 3D plotting. In our case, we will be plotting the aruco marker as a gray square with its corners marked following the color-code explained before. Additionally, the reference system of 'the scene'  will be also drawn (Red for X axis, Green for Y and blue for Z).

With that, the *scene* is plotted, now we need to plot the camera together with the trajectory it is following. To do so, we will create a list containing all the positions the camera has taken in the execution loop and, with that, a pinkish line (determining the trajectory) finishing with a dark pink ball (atual position) will be plotted.

Is important to consider that Matplotlib introduces some delay to the code in case we want to visualize it real time. However, although trajectory is plotted on the screen, an output video is generated and saved on a determined path in `.mp4`format.


## Results

The journey to arrive at this point has been really interesting, but not easy or fast. To make sure the steps we were taken were correctly made, the process was divided in smaller steps.

The first approach once we have understood the theory and made a first version of the code, was to evaluate if the camera position is being properly calculated using just **one image**. Result is shown below.

![Camera Position single image](https://github.com/brodgon/brodgon.github.io/blob/master/docs/camera_single_img2.png?raw=true)
**Camera position estimation based on a single image. Results are shown from two views)**

Making sure evrything was working, the next step is to move towards video. Although this may be difficult, its implementation is pretty straightfoward since we just have to understand the video as a collection of images. In this sense, two scenarios were tested: using a 'pre-recorded video' and performing 'real-time tracking'. Results for the first ones are shown below (gif format, accelerated view and lower resolution) or by clicking [on this link](https://urjc-my.sharepoint.com/:v:/g/personal/b_rodriguezg_2018_alumnos_urjc_es/ER8qg7nbMSxBusoOIOnYXCUBhsPzkkJhSOIzE-hXEF-Ulg?e=DFywzR) (original video).

![Camera position](https://github.com/brodgon/brodgon.github.io/blob/master/docs/gif_autoloc_vid.gif?raw=true)


Real-time tracking was, however, a bit more complicated. Since we are using a mobile phone's camera, we need to 'connect it' to our computer in order to pair them. For that IPWebcam application (available for Android) was used. This application provides a connection between the camera placed in our mobile phone and the computer using IP communications. After giving permission to the app to acces our camera, we will launch a server on or mobile and copy the link in the code followed by */shot.jpg*. This will allow our code to access our camera and localize itself based on the marker. Results are shown below (gif format, lower reso and faster) or by (clicking on this link)[https://urjc-my.sharepoint.com/:v:/g/personal/b_rodriguezg_2018_alumnos_urjc_es/EQmfHrRnRH5EjjmYkSl9hAIBMSaCd0Xkq1lQLqs0JL7ekg?e=X3NEaG] (original).

![Camera position webcam](https://github.com/brodgon/brodgon.github.io/blob/master/docs/gif_autoloc_webcam.gif?raw=true)

## Discusion and improvements

As seen on the above results, a pretty good localization has been achived; however, the system still have some limitations, specially regarding times and real-time processing. When working with pre-recorded videos, results are processed really fast, resulting in smooth results (although we are introducing delay with matplotlib). However, real-time implementation with the webcam is showing abrupt results, probably associated to connections issues. Therefore, a little more work may be done in this line.

Additionally, to make visualization funnier, I've been trying to project the aruco marker on the scene, however, lots of problems were arising.

Nevertheless, the objectives of the third practice of the Robotic Vision course have been fullfilled, since results have been obtained and theory has been understood.

For any coments or further doubts, don’t hesitate to contact me, I will be pleased to help!!

This is the end of the journey for the practices of the robotic course; however, this does not means we are going to stop learning or imrpoving! Stay tunned!


> **Note:** This post was created and last edited on 2023.05.17.
